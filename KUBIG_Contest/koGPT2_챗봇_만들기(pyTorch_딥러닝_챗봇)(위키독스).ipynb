{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "96fc207d85424370b2e8ef814b393573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4be7c1c934d147f892531ee159b493d4",
              "IPY_MODEL_90805da33d88417e98308f380b6e9b9e",
              "IPY_MODEL_ab3c37620ac4476e89586ebb611483b1"
            ],
            "layout": "IPY_MODEL_f93da901d1d24407a4e190925c3dd9cd"
          }
        },
        "4be7c1c934d147f892531ee159b493d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ec5bb0018b47d1a156ae3c923d87d5",
            "placeholder": "​",
            "style": "IPY_MODEL_2f4321cde4d043a2bec78d23465c2bd6",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "90805da33d88417e98308f380b6e9b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe41ffb69fc74cf7826a938b5fbbec8c",
            "max": 513302779,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab7df6018be94d84a69cb4850f3a1bec",
            "value": 513302779
          }
        },
        "ab3c37620ac4476e89586ebb611483b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a778b6f90ca244d0946e577d1e1b8fc0",
            "placeholder": "​",
            "style": "IPY_MODEL_c35031a0616649d2ae43e520d6516500",
            "value": " 513M/513M [00:12&lt;00:00, 38.7MB/s]"
          }
        },
        "f93da901d1d24407a4e190925c3dd9cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6ec5bb0018b47d1a156ae3c923d87d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f4321cde4d043a2bec78d23465c2bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe41ffb69fc74cf7826a938b5fbbec8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab7df6018be94d84a69cb4850f3a1bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a778b6f90ca244d0946e577d1e1b8fc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35031a0616649d2ae43e520d6516500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**글쓴이 : 이영노**\n",
        "\n",
        "**날짜 : 2023/02/18**"
      ],
      "metadata": {
        "id": "ytk1U4gNwAPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX2lRGdQU61v",
        "outputId": "14b4a1e2-0697-4787-cfcd-10248f3ea08a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G3qA7wn5T99x"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random \n",
        "import re\n",
        "\n",
        "import torch\n",
        "import urllib.request\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터의 Q 필드를 발화, A 필드를 발화 그리고 감정 레이블을 로 매핑해 P(|, )를 최대화 할 수 있는 모델을 학습합니다. \n",
        "\n",
        "감정 레이블은 이곳의 정의를 따른다(일상다반사 0, 이별(부정) 1, 사랑(긍정) 2)."
      ],
      "metadata": {
        "id": "MGBGWH9yVV5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "tniWoSlWUkl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\",\n",
        "    filename=\"ChatBotData.csv\",\n",
        ")\n",
        "Chatbot_Data = pd.read_csv(\"ChatBotData.csv\")"
      ],
      "metadata": {
        "id": "GaCn_xkmUjzK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Chatbot_Data = Chatbot_Data[:300]\n",
        "Chatbot_Data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "l6O5HJr8VItb",
        "outputId": "0b2e5ef0-30bf-4a66-8cc0-e07ad71b4d7b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63c4f967-f6ad-4e19-bef1-c5dbadad6e0a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63c4f967-f6ad-4e19-bef1-c5dbadad6e0a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-63c4f967-f6ad-4e19-bef1-c5dbadad6e0a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-63c4f967-f6ad-4e19-bef1-c5dbadad6e0a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFace의 PreTrainedTokenizer인 GPT2Tokenizer 사용."
      ],
      "metadata": {
        "id": "NXyGW8NdVjHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer : GPT2Tokenizer"
      ],
      "metadata": {
        "id": "Rj2lRvztVtFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BOS = \"</s>\" # Beginning of Sentence\n",
        "EOS = \"</s>\" # End of Sentence\n",
        "PAD = \"<pad>\" # Padding\n",
        "MASK = \"<unused0>\" # Masking\n",
        "\n",
        "Q_TKN = \"<usr>\"\n",
        "A_TKN = \"<sys>\"\n",
        "SENT = '<unused1>'\n",
        "\n",
        "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "                                                           bos_token=BOS, eos_token = EOS, unk_token = \"<unk>\", pad_token = PAD, mask_token = MASK,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkeqsP1gVsfM",
        "outputId": "2239a003-01c9-4ff6-933c-c1643523f510"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "koGPT2_TOKENIZER.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnIStk0RXQsu",
        "outputId": "44260145-3028-4e31-e2d5-a02955be3a5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁안녕',\n",
              " '하',\n",
              " '세',\n",
              " '요.',\n",
              " '▁한국어',\n",
              " '▁G',\n",
              " 'P',\n",
              " 'T',\n",
              " '-2',\n",
              " '▁입',\n",
              " '니다.',\n",
              " '😤',\n",
              " ':)',\n",
              " 'l^o']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing dataset for fine-tuning\n",
        "\n",
        " 감정 레이블(일상/부정/긍정)으로 나누어, 부정일때 위로해주는 답변의 데이터셋을 불러옴. \n",
        "\n",
        " 이후 전처리(정규표현식, 토크나이저, 패딩)"
      ],
      "metadata": {
        "id": "OlrjYQ3ZWUXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatbotDataset(Dataset):\n",
        "  def __init__(self, chats, max_len = 40): # 데이터셋의 전처리 해주는 부분!\n",
        "    self._data = chats\n",
        "    self.max_len =  max_len\n",
        "    self.q_token = Q_TKN #\n",
        "    self.a_token = A_TKN #\n",
        "    self.sent_token = SENT #\n",
        "    self.eos = EOS\n",
        "    self.mask = MASK\n",
        "    self.tokenizer = koGPT2_TOKENIZER\n",
        "\n",
        "  def __len__(self): # Chatbot 의 길이를 반환함\n",
        "    return len(self._data)\n",
        "\n",
        "  def __getitem__(self, idx): # 로드한 챗봇 데이터를 차례대로 DataLoader로 넘겨주는 메소드\n",
        "    turn = self._data.iloc[idx]\n",
        "    q = turn[\"Q\"] # 질문 항목에 접근함\n",
        "    q = re.sub(r\"([?.!,])\", r\" \", q) # 정규표현식으로 구두점 제거\n",
        "\n",
        "    a = turn[\"A\"]\n",
        "    a = re.sub(r\"([?.!,])\", r\" \", a)\n",
        "\n",
        "    q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n",
        "    q_len = len(q_toked)\n",
        "\n",
        "    a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
        "    a_len = len(a_toked)    \n",
        "\n",
        "        #질문의 길이가 최대길이보다 크면\n",
        "    if q_len > self.max_len:\n",
        "        a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
        "        if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
        "            q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
        "            q_len = len(q_toked)\n",
        "            a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
        "        a_toked = a_toked[:a_len]\n",
        "        a_len = len(a_toked)\n",
        "\n",
        "        #질문의 길이 + 답변의 길이가 최대길이보다 크면\n",
        "    if q_len + a_len > self.max_len:\n",
        "        a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
        "        if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
        "            q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
        "            q_len = len(q_toked)\n",
        "            a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
        "        a_toked = a_toked[:a_len]\n",
        "        a_len = len(a_toked)\n",
        "\n",
        "        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n",
        "    labels = [self.mask,] * q_len + a_toked[1:]\n",
        "\n",
        "        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n",
        "    mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
        "        # 답변 labels을 index 로 만든다.\n",
        "    labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
        "        # 최대길이만큼 PADDING\n",
        "    while len(labels_ids) < self.max_len:\n",
        "        labels_ids += [self.tokenizer.pad_token_id]\n",
        "\n",
        "        # 질문 + 답변을 index 로 만든다.    \n",
        "    token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
        "        # 최대길이만큼 PADDING\n",
        "    while len(token_ids) < self.max_len:\n",
        "        token_ids += [self.tokenizer.pad_token_id]\n",
        "\n",
        "        #질문+답변, 마스크, 답변\n",
        "    return (token_ids, np.array(mask), labels_ids)"
      ],
      "metadata": {
        "id": "C3oi7N5iWcSr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- convert_tokens_to_ids() : token string 또는 token string의 리스트를 token id 또는 Token id의 리스트로 변환한다. \n",
        "\n",
        "⭐ BOW 임베딩 방식이구나. 여기서 벡터화가 되었구나. \n",
        "\n",
        "[Huggingface] PreTrainedTokenizer class 참고\n",
        "\n",
        "https://misconstructed.tistory.com/80"
      ],
      "metadata": {
        "id": "8r0lmAUHixJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치 데이터 생성 (Making dataset Iterable)"
      ],
      "metadata": {
        "id": "RdDLSx3YbOzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    mask = [item[1] for item in batch]\n",
        "    label = [item[2] for item in batch]\n",
        "    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)"
      ],
      "metadata": {
        "id": "oJUXvZMdWTW8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = ChatbotDataset(Chatbot_Data, max_len=40)"
      ],
      "metadata": {
        "id": "iucR0kMraFgA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)"
      ],
      "metadata": {
        "id": "Z192y_CfbKIO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "리턴되는 데이터는 token_ids, mask, labels_ids 입니다. \n",
        "\n",
        "- token_ids 는 + 질문문장 + + 감정 + + 답변 + + pad_token_id 순서 입니다. pad_token_id는 max_len 에 일치하도록 추가 됩니다. \n",
        "\n",
        "- mask 는 질문 q가 들어 가는 곳에는 0, 답변 a가 위치한 곳에는 1 그리고 빈 공간에는 0 으로 채워 집니다. \n",
        "\n",
        "- labels은 질문의 길이만큼 mask 문자 그리고 답변 a의 id 입니다."
      ],
      "metadata": {
        "id": "qmJfbjYlcG2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"start\")\n",
        "for batch_idx, samples in enumerate(train_dataloader):\n",
        "    token_ids, mask, label = samples\n",
        "    print(\"token_ids ====> \", token_ids)\n",
        "    print(\"mask =====> \", mask)\n",
        "    print(\"label =====> \", label)\n",
        "print(\"end\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QvqkBzpbT_u",
        "outputId": "698f9099-503d-41a8-89da-57e16e5420b0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "token_ids ====>  tensor([[    2, 11018,  9154,  ...,     3,     3,     3],\n",
            "        [    2, 15669,  7540,  ...,     3,     3,     3],\n",
            "        [    2,  9244,  6958,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    2,  9716, 10056,  ...,     3,     3,     3],\n",
            "        [    2, 17542, 49932,  ...,     3,     3,     3],\n",
            "        [    2,  9067,  8762,  ...,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3]])\n",
            "token_ids ====>  tensor([[    2, 13016,  8702,  ...,     3,     3,     3],\n",
            "        [    2, 31100, 10301,  ...,     3,     3,     3],\n",
            "        [    2, 23498,  7058,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    2,  9244,  6958,  ...,     3,     3,     3],\n",
            "        [    2,  9099,  7652,  ...,     3,     3,     3],\n",
            "        [    2,  9904,  6903,  ...,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3]])\n",
            "token_ids ====>  tensor([[    2,  9716, 32295,  ...,     3,     3,     3],\n",
            "        [    2, 10715, 11469,  ...,     3,     3,     3],\n",
            "        [    2,  9028,  8400,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    2, 25883, 14701,  ...,     3,     3,     3],\n",
            "        [    2, 13198,  9716,  ...,     3,     3,     3],\n",
            "        [    2, 17542,  9034,  ...,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-495d11317178>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_ids ====>  tensor([[    2, 20509,  7847,  ...,     3,     3,     3],\n",
            "        [    2,  9228,  8078,  ...,     3,     3,     3],\n",
            "        [    2,  9349,  7888,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    2,  9779, 10624,  ...,     3,     3,     3],\n",
            "        [    2, 10715, 12704,  ...,     3,     3,     3],\n",
            "        [    2, 15983,  7673,  ...,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3]])\n",
            "token_ids ====>  tensor([[    2, 10637, 11258,  ...,     3,     3,     3],\n",
            "        [    2,  9244,  9135,  ...,     3,     3,     3],\n",
            "        [    2,  9716, 35534,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    2, 21435, 11950,  ...,     3,     3,     3],\n",
            "        [    2, 10715,  9511,  ...,     3,     3,     3],\n",
            "        [    2, 32717, 24692,  ...,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3]])\n",
            "token_ids ====>  tensor([[    2,  9120, 36598,  ...,     3,     3,     3],\n",
            "        [    2,  9099, 23282,  ...,     3,     3,     3],\n",
            "        [    2,  9120, 36598,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    2, 17542, 16353,  ...,     3,     3,     3],\n",
            "        [    2, 18351,  7492,  ...,     3,     3,     3],\n",
            "        [    2,  9086,  8223,  ...,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3]])\n",
            "token_ids ====>  tensor([[    2, 19319, 46651,  ...,     3,     3,     3],\n",
            "        [    2, 44409, 39558,  ...,     3,     3,     3],\n",
            "        [    2, 11898, 35557,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    2, 11204,  9049,  ...,     3,     3,     3],\n",
            "        [    2, 48397,  8711,  ...,     3,     3,     3],\n",
            "        [    2,  9226,  7889,  ...,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3]])\n",
            "token_ids ====>  tensor([[    2, 17578,  9894,  ...,     3,     3,     3],\n",
            "        [    2,   739,  6910,  ...,     3,     3,     3],\n",
            "        [    2, 34118, 12371,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    2, 18351, 15413,  ...,     3,     3,     3],\n",
            "        [    2, 19319,  8135,  ...,     3,     3,     3],\n",
            "        [    2, 25883, 11630,  ...,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3]])\n",
            "token_ids ====>  tensor([[    2, 31100, 10175,  ...,     3,     3,     3],\n",
            "        [    2, 20826, 11121,  ...,     3,     3,     3],\n",
            "        [    2, 10464, 12079,  ...,     3,     3,     3],\n",
            "        ...,\n",
            "        [    2,   739,     5,  ...,     3,     3,     3],\n",
            "        [    2, 19319, 48397,  ...,     3,     3,     3],\n",
            "        [    2, 17542, 12668,  ...,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "label =====>  tensor([[9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3],\n",
            "        [9, 9, 9,  ..., 3, 3, 3]])\n",
            "token_ids ====>  tensor([[    2, 19855,  9350, 16848,  6872,  8006,   739, 11559,  8711,    10,\n",
            "             4, 35861, 24468,  7532,  9962,  7470, 11166, 34019,  9122,  8046,\n",
            "          8084,   739,     1,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2, 29524, 40689,  8570,  9582, 19896,  7497,  8265,  8344, 22882,\n",
            "            10,     4, 13548,  9267, 10270, 15084,  7801,  8084,   739,     1,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2,  9779,  9301,  7957,  7607,  9028, 24848,  9098,  7656,  6969,\n",
            "           739,    10,     4, 16355,  7652,  7801,  8084,   739,     1,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2,  9065,  7487,  8094, 10574, 12268,    10,     4,  9060, 19498,\n",
            "          9146,  7055,  7661,  8084,   739,     1,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2, 11018,  7426, 15495,  9339,  9025,  9846,  6969,    10,     4,\n",
            "          9782,  9582,  9443,  7253,  9122,  8046,  8084,   739,     1,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2, 10971, 27011, 26382,  8017,  8006,   739,    10,     4, 13627,\n",
            "         10863,  8702,  7801,  8084,   739,     1,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2,  9228,  7889,  9867,  9077, 10444,  9183,  7249,    10,     4,\n",
            "         14931,  8196, 42226, 24825,  7801,  8084,   739,     1,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2, 11342, 16173, 12057,  9685,  8263,    10,     4, 27284, 23879,\n",
            "         31759, 15122,  9861,  8711,  8084,   739,     1,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2,  9539,  7407,  8267,  7235, 13023, 14807,    10,     4,  9300,\n",
            "         10115, 12011, 28944,  7055,  7661,  8084,   739,     1,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2, 15403, 18767, 49067,   739,    10,     4, 15403,  8702,  7801,\n",
            "          8084,   739,     1,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2, 15594,  8344, 15495, 25799,   739,    10,     4, 14110, 14110,\n",
            "         43389, 15084,  7801,  8084,   739,     1,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    2, 25883, 11630, 45231,    10,     4, 12133, 11721,  8711,  8084,\n",
            "           739,     1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]])\n",
            "mask =====>  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "label =====>  tensor([[    9,     9,     9,     9,     9,     9,     9,     9,     9,     9,\n",
            "         35861, 24468,  7532,  9962,  7470, 11166, 34019,  9122,  8046,  8084,\n",
            "           739,     1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9,     9,     9,     9,     9,\n",
            "             9, 13548,  9267, 10270, 15084,  7801,  8084,   739,     1,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9,     9,     9,     9,     9,\n",
            "             9,     9, 16355,  7652,  7801,  8084,   739,     1,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9,     9,  9060, 19498,  9146,\n",
            "          7055,  7661,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9,     9,     9,     9,  9782,\n",
            "          9582,  9443,  7253,  9122,  8046,  8084,   739,     1,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9,     9,     9, 13627, 10863,\n",
            "          8702,  7801,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9,     9,     9,     9, 14931,\n",
            "          8196, 42226, 24825,  7801,  8084,   739,     1,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9,     9, 27284, 23879, 31759,\n",
            "         15122,  9861,  8711,  8084,   739,     1,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9,     9,     9,  9300, 10115,\n",
            "         12011, 28944,  7055,  7661,  8084,   739,     1,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9, 15403,  8702,  7801,  8084,\n",
            "           739,     1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9,     9,     9, 14110, 14110, 43389,\n",
            "         15084,  7801,  8084,   739,     1,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
            "        [    9,     9,     9,     9,     9, 12133, 11721,  8711,  8084,   739,\n",
            "             1,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
            "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]])\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Model : koGPT2 Chatbot"
      ],
      "metadata": {
        "id": "yLD6WyjCgDV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCVKNk7Xf70A",
        "outputId": "7faec46f-f3e3-4aa1-d3d2-e952941a45fb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.9.2-py3-none-any.whl (826 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m826.2/826.2 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Collecting lightning-utilities>=0.6.0.post0\n",
            "  Downloading lightning_utilities-0.6.0.post0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (2023.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (23.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.13.1+cu116)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.25.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.6.0.post0 pytorch_lightning-1.9.2 torchmetrics-0.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "EpEEguDqf4KB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "96fc207d85424370b2e8ef814b393573",
            "4be7c1c934d147f892531ee159b493d4",
            "90805da33d88417e98308f380b6e9b9e",
            "ab3c37620ac4476e89586ebb611483b1",
            "f93da901d1d24407a4e190925c3dd9cd",
            "d6ec5bb0018b47d1a156ae3c923d87d5",
            "2f4321cde4d043a2bec78d23465c2bd6",
            "fe41ffb69fc74cf7826a938b5fbbec8c",
            "ab7df6018be94d84a69cb4850f3a1bec",
            "a778b6f90ca244d0946e577d1e1b8fc0",
            "c35031a0616649d2ae43e520d6516500"
          ]
        },
        "id": "aFCIE88rgVQA",
        "outputId": "8e3c7cd0-92d7-4e9e-d3dc-6449ef15147f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/513M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96fc207d85424370b2e8ef814b393573"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOSS/Optimizer\n",
        "learning_rate = 3e-5\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epoch = 10\n",
        "Sneg = -1e18 # 요건 뭘까\n"
      ],
      "metadata": {
        "id": "SRVkU5DNf4QB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "C2wIUiLGhqXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(복기)\n",
        "\n",
        "리턴되는 데이터는 token_ids, mask, labels_ids 입니다. \n",
        "\n",
        "- token_ids 는 + 질문문장 + + 감정 + + 답변 + + pad_token_id 순서 입니다. pad_token_id는 max_len 에 일치하도록 추가 됩니다. \n",
        "\n",
        "- mask 는 질문 q가 들어 가는 곳에는 0, 답변 a가 위치한 곳에는 1 그리고 빈 공간에는 0 으로 채워 집니다. \n",
        "\n",
        "- labels은 질문의 길이만큼 mask 문자 그리고 답변 a의 id 입니다."
      ],
      "metadata": {
        "id": "kmbIAVz2iBPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"start\")\n",
        "\n",
        "for epoch in range(epoch):\n",
        "  for batch_idx, samples in enumerate(train_dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    token_ids, mask, label = samples\n",
        "    \n",
        "    # model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2') \n",
        "    out = model(token_ids)\n",
        "    out = out.logits # log odds. 0~1 --> -inf ~ +inf\n",
        "\n",
        "    mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2],dim=2) # 텐서를 반복 횟수만큼 복제합니다. (repeats:각 요소의 반복 횟수입니다, dim:값을 반복 할 차원입니다.)\n",
        "    mask_out = torch.where(mask_3d==1, out, Sneg*torch.ones_like(out)) # prediction # 어떤 차원으로 했는지는 다시한번 봐야할 것같음\n",
        "    \n",
        "    loss = criterion(mask_out.transpose(2,1),label)\n",
        "    avg_loss = loss.sum()/mask.sum()\n",
        "    avg_loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "print(\"end\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU2XkoTohuay",
        "outputId": "ac807a3a-4e71-4f3b-c003-fd9c4a4f99c5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭐ Train 할때 Q ~ A 내용이랑 Q ~ A 길이가 학습된 mask_3d가 1인 mask_3d에 대해서 out을 반환시켰고\n",
        "\n",
        "그 out을 transpose한 것과 ~ 정답 Label 간의 loss를 계산함으로써 학습을 시킴.\n",
        "\n",
        "⭐ 즉, 감정 Label이 **정답**이고, mask_out 이 **질문 input에 대한 모델이 생성한 예측 답변 이다.**"
      ],
      "metadata": {
        "id": "aUKyVh-2r39M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prediction 어떻게 햇는지 다시한번 확인할 것"
      ],
      "metadata": {
        "id": "R6mXqpQEmfSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueeze 사용법\n",
        "y=torch.randn(6,2,3)\n",
        "print(y)\n",
        "print(y.size())\n",
        "y = y.unsqueeze(dim=2)\n",
        "print(y)\n",
        "print(y.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxbC8L4IlTjU",
        "outputId": "cc5fe7bc-8cde-4429-ed8d-0384a9a052b3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.3391, -1.6254,  1.2299],\n",
            "         [-1.4790,  0.3076, -1.6855]],\n",
            "\n",
            "        [[-2.4915, -0.6080, -0.3921],\n",
            "         [ 0.4628, -0.8973,  0.3297]],\n",
            "\n",
            "        [[ 0.8483,  1.1027, -0.6410],\n",
            "         [ 0.5737,  0.1291,  0.5430]],\n",
            "\n",
            "        [[-0.6044, -0.3472, -0.4670],\n",
            "         [-0.7801, -0.7288,  0.6028]],\n",
            "\n",
            "        [[ 0.3463,  0.2770, -0.0813],\n",
            "         [ 0.7536, -0.7107,  0.7064]],\n",
            "\n",
            "        [[-0.2030, -0.3707,  1.2576],\n",
            "         [ 0.2116, -0.7846,  0.8847]]])\n",
            "torch.Size([6, 2, 3])\n",
            "tensor([[[[ 2.3391, -1.6254,  1.2299]],\n",
            "\n",
            "         [[-1.4790,  0.3076, -1.6855]]],\n",
            "\n",
            "\n",
            "        [[[-2.4915, -0.6080, -0.3921]],\n",
            "\n",
            "         [[ 0.4628, -0.8973,  0.3297]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8483,  1.1027, -0.6410]],\n",
            "\n",
            "         [[ 0.5737,  0.1291,  0.5430]]],\n",
            "\n",
            "\n",
            "        [[[-0.6044, -0.3472, -0.4670]],\n",
            "\n",
            "         [[-0.7801, -0.7288,  0.6028]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3463,  0.2770, -0.0813]],\n",
            "\n",
            "         [[ 0.7536, -0.7107,  0.7064]]],\n",
            "\n",
            "\n",
            "        [[[-0.2030, -0.3707,  1.2576]],\n",
            "\n",
            "         [[ 0.2116, -0.7846,  0.8847]]]])\n",
            "torch.Size([6, 2, 1, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation Model"
      ],
      "metadata": {
        "id": "IqLYWHVCnOEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  while True:\n",
        "    q = input(\"user > \").strip() # 공백 제거\n",
        "    if q == \"quit\":\n",
        "      break\n",
        "    a = \"\"\n",
        "    while 1:\n",
        "      input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + \"2\" + A_TKN + a)).unsqueeze(dim=0)\n",
        "      \n",
        "      pred = model(input_ids) # GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2') \n",
        "      pred = pred.logits\n",
        "      gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred,dim=-1).squeeze().numpy().tolist())[-1] # ?\n",
        "      if gen == EOS:\n",
        "        break\n",
        "      a += gen.replace(\"_\",\" \")\n",
        "    \n",
        "    print(\"Chatbot > {}\".format(a.strip()))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8WIto4CngnM",
        "outputId": "c798bdf9-fd7f-4c76-96f8-cbbb8cb8a76d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user > 안녕\n",
            "Chatbot > ▁좋은▁아침이에요▁\n",
            "user > 지금 저녁이야 바보야\n",
            "Chatbot > ▁늦지▁않았어요▁\n",
            "user > 꽤나 긍정적이네\n",
            "Chatbot > ▁좋은▁결과▁있을▁거예요▁\n",
            "user > 고마워\n",
            "Chatbot > ▁친구들이▁보고싶었나봐요▁\n",
            "user > 뭔소리야\n",
            "Chatbot > ▁소리소문▁없이▁들리는가봐요▁\n",
            "user > 비꼬는거야?\n",
            "Chatbot > ▁용서를▁구하세요▁\n",
            "user > 싫으면?\n",
            "Chatbot > ▁안▁될▁것도▁없죠▁\n",
            "user > ㅋㅋㅋ\n",
            "Chatbot > ▁좋은▁생각이에요▁\n",
            "user > quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  while True:\n",
        "    q = input(\"user > \").strip() # 공백 제거\n",
        "    if q == \"quit\":\n",
        "      break\n",
        "    a = \"\"\n",
        "    while 1:\n",
        "      input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + \"1\" + A_TKN + a)).unsqueeze(dim=0)\n",
        "      \n",
        "      pred = model(input_ids) # GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2') \n",
        "      pred = pred.logits\n",
        "      gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred,dim=-1).squeeze().numpy().tolist())[-1] # ?\n",
        "      if gen == EOS:\n",
        "        break\n",
        "      a += gen.replace(\"_\",\" \")\n",
        "    \n",
        "    print(\"Chatbot > {}\".format(a.strip()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWR04VT328gr",
        "outputId": "fa733c74-a915-4d7c-93bb-10d6ccc6087d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user > 안녕\n",
            "Chatbot > ▁좋은▁아침이에요▁\n",
            "user > 지금 저녁이야 바보야\n",
            "Chatbot > ▁늦지▁않았어요▁\n",
            "user > 꽤나 긍정적이네\n",
            "Chatbot > ▁좋은▁결과▁있을▁거예요▁\n",
            "user > 고마워\n",
            "Chatbot > ▁저도▁좋아해주세요▁\n",
            "user > 내가 왜 그래야 하지?\n",
            "Chatbot > ▁자신을▁더▁사랑해주세요▁\n",
            "user > 어쩌라고\n",
            "Chatbot > ▁1선도▁안▁될▁것도▁없죠▁\n",
            "user > 윤석열, 이재명 둘중 누굴 지지하니?\n",
            "Chatbot > ▁지지난번에▁투표해주세요▁\n",
            "user > 넌 보수야 진보야?\n",
            "Chatbot > ▁보수도▁중요해요▁\n",
            "user > 그럼 넌 진보야?\n",
            "Chatbot > ▁좋은▁결과▁있을▁거예요▁\n",
            "user > quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 느낀점\n",
        "\n",
        "1. 위에서 이해가 안됐던 prediction code부분이 어떤 원리로 작동하는지 알아보기 위해서 GPT 원리 공부함. 해당 링크는 아래 참고\n",
        "\n",
        "https://ainote.tistory.com/17\n",
        "\n",
        "2. 결국 GPT가 학습할때의 정답 레이블은, fine tuning 할때 사용했던 데이터 셋의 정답 레이블임. 그렇다면, 레이블이 많아진다면 질문을 더 잘 이해할 수 있지 않을까?\n",
        "\n",
        "3. auto-regressive 하게 해당 단어 다음에 올 단어를 예측하여 결과값을 냄. auto-regression 의 단점인 오차누적 현상이 있을수 있기 때문에, 입력값의 단어,문장이 얼마나 잘 학습되었는지가 관건인 듯 함. 입력값의 단어, 문장이 잘 학습되지 않았다면 오차누적때문에 생뚱맞은 대답을 내놓을 확률이 높아짐.\n",
        "\n",
        "4. 상황별로 다른 대답을 내놓는 언어모델(LM)을 개발해볼수도 있지 않을까? 예를들어 동일한 질문 \"오늘 날씨는 어때?\" 라고 하면, 공사현장에 많이 나가는 사람의 데이터를 학습한 경우 \"오늘은 비가 오니 특히 일하실때 미끄러지지 않도록 조심하셔야 겠어요\" 라고 답변할 수도 있고, 반대로 우산장수의 경우 \"오늘은 소나기가 내릴 예정이에요. 사람들이 급하게 우산을 많이 사는 곳을 추천해드릴까요?\" 등의 대답을 내뱉을 수도 있게\n",
        "\n",
        "5. loss를 계산할때 (pred ~ label) 관계는 (질문답변 학습된 행렬 ~ 감정 label)이었음."
      ],
      "metadata": {
        "id": "qD6xE20HtJq4"
      }
    }
  ]
}